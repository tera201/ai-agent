{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "b'C:\\\\Users\\\\tera2\\\\AppData\\\\Local\\\\Temp\\\\try_flags_f9qy_myb.c:4:19: fatal error: cudnn.h: No such file or directory\\r\\ncompilation terminated.\\r\\n'\n",
      "Preallocating 716/1024 Mb (0.700000) on cuda0\n",
      "Mapped name None to device cuda0: GeForce GTX 460 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.reset()\n",
    "goal_steps = 200\n",
    "score_requirement = -198\n",
    "intial_games = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_a_random_game_first():\n",
    "    for step_index in range(goal_steps):\n",
    "#         env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        print(\"Step {}:\".format(step_index))\n",
    "        print(\"action: {}\".format(action))\n",
    "        print(\"observation: {}\".format(observation))\n",
    "        print(\"reward: {}\".format(reward))\n",
    "        print(\"done: {}\".format(done))\n",
    "        print(\"info: {}\".format(info))\n",
    "        if done:\n",
    "            break\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "action: 2\n",
      "observation: [-4.33893038e-01  3.37740900e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 1:\n",
      "action: 2\n",
      "observation: [-0.43322     0.00067304]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 2:\n",
      "action: 2\n",
      "observation: [-0.43221652  0.00100347]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 3:\n",
      "action: 2\n",
      "observation: [-0.43088987  0.00132666]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 4:\n",
      "action: 1\n",
      "observation: [-0.43024959  0.00064027]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 5:\n",
      "action: 2\n",
      "observation: [-0.42930032  0.00094927]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 6:\n",
      "action: 0\n",
      "observation: [-0.4300489  -0.00074857]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 7:\n",
      "action: 1\n",
      "observation: [-0.43148992 -0.00144102]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 8:\n",
      "action: 1\n",
      "observation: [-0.433613   -0.00212308]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 9:\n",
      "action: 2\n",
      "observation: [-0.43540281 -0.00178981]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 10:\n",
      "action: 2\n",
      "observation: [-0.4368464  -0.00144359]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 11:\n",
      "action: 2\n",
      "observation: [-0.43793331 -0.00108691]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 12:\n",
      "action: 2\n",
      "observation: [-0.43865566 -0.00072235]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 13:\n",
      "action: 2\n",
      "observation: [-4.39008211e-01 -3.52551856e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 14:\n",
      "action: 1\n",
      "observation: [-0.4399884  -0.00098019]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 15:\n",
      "action: 2\n",
      "observation: [-0.44058912 -0.00060072]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 16:\n",
      "action: 2\n",
      "observation: [-4.40805994e-01 -2.16873804e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 17:\n",
      "action: 0\n",
      "observation: [-0.44263745 -0.00183145]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 18:\n",
      "action: 2\n",
      "observation: [-0.44407016 -0.00143271]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 19:\n",
      "action: 2\n",
      "observation: [-0.4450937  -0.00102353]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 20:\n",
      "action: 0\n",
      "observation: [-0.44770059 -0.00260689]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 21:\n",
      "action: 1\n",
      "observation: [-0.45087181 -0.00317122]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 22:\n",
      "action: 2\n",
      "observation: [-0.45358417 -0.00271236]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 23:\n",
      "action: 2\n",
      "observation: [-0.45581779 -0.00223362]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 24:\n",
      "action: 1\n",
      "observation: [-0.45855627 -0.00273848]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 25:\n",
      "action: 1\n",
      "observation: [-0.46177947 -0.00322321]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 26:\n",
      "action: 2\n",
      "observation: [-0.46446367 -0.0026842 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 27:\n",
      "action: 1\n",
      "observation: [-0.46758906 -0.00312539]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 28:\n",
      "action: 1\n",
      "observation: [-0.47113255 -0.00354349]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 29:\n",
      "action: 0\n",
      "observation: [-0.47606791 -0.00493536]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 30:\n",
      "action: 1\n",
      "observation: [-0.48135855 -0.00529064]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 31:\n",
      "action: 1\n",
      "observation: [-0.48696514 -0.00560659]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 32:\n",
      "action: 0\n",
      "observation: [-0.49384593 -0.00688079]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 33:\n",
      "action: 2\n",
      "observation: [-0.49994957 -0.00610364]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 34:\n",
      "action: 2\n",
      "observation: [-0.50523043 -0.00528086]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 35:\n",
      "action: 0\n",
      "observation: [-0.51164899 -0.00641855]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 36:\n",
      "action: 0\n",
      "observation: [-0.51915714 -0.00750816]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 37:\n",
      "action: 2\n",
      "observation: [-0.52569861 -0.00654147]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 38:\n",
      "action: 1\n",
      "observation: [-0.53222433 -0.00652572]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 39:\n",
      "action: 0\n",
      "observation: [-0.53968537 -0.00746104]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 40:\n",
      "action: 0\n",
      "observation: [-0.5480258  -0.00834043]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 41:\n",
      "action: 0\n",
      "observation: [-0.5571832  -0.00915739]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 42:\n",
      "action: 2\n",
      "observation: [-0.56508913 -0.00790594]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 43:\n",
      "action: 0\n",
      "observation: [-0.5736847  -0.00859556]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 44:\n",
      "action: 0\n",
      "observation: [-0.58290603 -0.00922133]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 45:\n",
      "action: 0\n",
      "observation: [-0.5926849  -0.00977887]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 46:\n",
      "action: 1\n",
      "observation: [-0.60194932 -0.00926442]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 47:\n",
      "action: 0\n",
      "observation: [-0.61163151 -0.00968219]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 48:\n",
      "action: 0\n",
      "observation: [-0.62166111 -0.01002959]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 49:\n",
      "action: 1\n",
      "observation: [-0.6309658  -0.00930469]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 50:\n",
      "action: 1\n",
      "observation: [-0.63947909 -0.00851329]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 51:\n",
      "action: 2\n",
      "observation: [-0.64614067 -0.00666159]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 52:\n",
      "action: 2\n",
      "observation: [-0.65090376 -0.00476309]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 53:\n",
      "action: 1\n",
      "observation: [-0.6547351  -0.00383134]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 54:\n",
      "action: 2\n",
      "observation: [-0.65660809 -0.00187299]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 55:\n",
      "action: 0\n",
      "observation: [-0.65850978 -0.00190168]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 56:\n",
      "action: 1\n",
      "observation: [-0.65942703 -0.00091725]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 57:\n",
      "action: 1\n",
      "observation: [-6.59353525e-01  7.35018187e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 58:\n",
      "action: 0\n",
      "observation: [-6.59289776e-01  6.37488782e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 59:\n",
      "action: 1\n",
      "observation: [-0.65823622  0.00105356]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 60:\n",
      "action: 1\n",
      "observation: [-0.65620011  0.0020361 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 61:\n",
      "action: 2\n",
      "observation: [-0.65219552  0.00400459]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 62:\n",
      "action: 2\n",
      "observation: [-0.6462502   0.00594532]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 63:\n",
      "action: 0\n",
      "observation: [-0.64040562  0.00584459]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 64:\n",
      "action: 2\n",
      "observation: [-0.6327028   0.00770282]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 65:\n",
      "action: 0\n",
      "observation: [-0.62519623  0.00750657]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 66:\n",
      "action: 0\n",
      "observation: [-0.61793942  0.00725681]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 67:\n",
      "action: 2\n",
      "observation: [-0.60898447  0.00895495]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 68:\n",
      "action: 1\n",
      "observation: [-0.59939611  0.00958836]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 69:\n",
      "action: 1\n",
      "observation: [-0.58924415  0.01015196]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 70:\n",
      "action: 0\n",
      "observation: [-0.57960303  0.00964112]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 71:\n",
      "action: 1\n",
      "observation: [-0.56954385  0.01005918]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 72:\n",
      "action: 2\n",
      "observation: [-0.55814117  0.01140268]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 73:\n",
      "action: 2\n",
      "observation: [-0.54547989  0.01266128]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 74:\n",
      "action: 0\n",
      "observation: [-0.53365461  0.01182527]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 75:\n",
      "action: 1\n",
      "observation: [-0.52175393  0.01190068]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 76:\n",
      "action: 1\n",
      "observation: [-0.50986709  0.01188684]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 77:\n",
      "action: 0\n",
      "observation: [-0.4990832   0.01078389]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 78:\n",
      "action: 2\n",
      "observation: [-0.48748302  0.01160018]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 79:\n",
      "action: 1\n",
      "observation: [-0.47615317  0.01132985]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 80:\n",
      "action: 0\n",
      "observation: [-0.46617797  0.0099752 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 81:\n",
      "action: 0\n",
      "observation: [-0.45763129  0.00854668]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 82:\n",
      "action: 0\n",
      "observation: [-0.45057615  0.00705514]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 83:\n",
      "action: 1\n",
      "observation: [-0.44406431  0.00651184]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 84:\n",
      "action: 0\n",
      "observation: [-0.43914333  0.00492098]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 85:\n",
      "action: 0\n",
      "observation: [-0.43584901  0.00329432]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 86:\n",
      "action: 1\n",
      "observation: [-0.43320524  0.00264377]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 87:\n",
      "action: 1\n",
      "observation: [-0.43123115  0.0019741 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 88:\n",
      "action: 0\n",
      "observation: [-4.30940976e-01  2.90171107e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 89:\n",
      "action: 2\n",
      "observation: [-0.43033682  0.00060415]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 90:\n",
      "action: 0\n",
      "observation: [-0.43142304 -0.00108622]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 91:\n",
      "action: 2\n",
      "observation: [-0.43219181 -0.00076876]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 92:\n",
      "action: 2\n",
      "observation: [-0.43263756 -0.00044576]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 93:\n",
      "action: 1\n",
      "observation: [-0.43375709 -0.00111953]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 94:\n",
      "action: 0\n",
      "observation: [-0.43654231 -0.00278521]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 95:\n",
      "action: 1\n",
      "observation: [-0.43997305 -0.00343074]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 96:\n",
      "action: 2\n",
      "observation: [-0.44302442 -0.00305137]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 97:\n",
      "action: 2\n",
      "observation: [-0.44567423 -0.00264981]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 98:\n",
      "action: 2\n",
      "observation: [-0.44790317 -0.00222894]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 99:\n",
      "action: 0\n",
      "observation: [-0.45169496 -0.00379179]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 100:\n",
      "action: 1\n",
      "observation: [-0.45602186 -0.0043269 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 101:\n",
      "action: 1\n",
      "observation: [-0.46085212 -0.00483026]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 102:\n",
      "action: 1\n",
      "observation: [-0.4661502  -0.00529808]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 103:\n",
      "action: 0\n",
      "observation: [-0.47287702 -0.00672682]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 104:\n",
      "action: 1\n",
      "observation: [-0.47998278 -0.00710576]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 105:\n",
      "action: 1\n",
      "observation: [-0.48741474 -0.00743195]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 106:\n",
      "action: 2\n",
      "observation: [-0.49411754 -0.0067028 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 107:\n",
      "action: 1\n",
      "observation: [-0.50104116 -0.00692362]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 108:\n",
      "action: 1\n",
      "observation: [-0.50813383 -0.00709267]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 109:\n",
      "action: 0\n",
      "observation: [-0.51634245 -0.00820862]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 110:\n",
      "action: 1\n",
      "observation: [-0.52460549 -0.00826304]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 111:\n",
      "action: 2\n",
      "observation: [-0.53186098 -0.00725549]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 112:\n",
      "action: 0\n",
      "observation: [-0.5400545  -0.00819353]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 113:\n",
      "action: 0\n",
      "observation: [-0.54912466 -0.00907016]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 114:\n",
      "action: 0\n",
      "observation: [-0.55900356 -0.0098789 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 115:\n",
      "action: 0\n",
      "observation: [-0.56961743 -0.01061387]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 116:\n",
      "action: 2\n",
      "observation: [-0.57888725 -0.00926982]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 117:\n",
      "action: 0\n",
      "observation: [-0.58874431 -0.00985706]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 118:\n",
      "action: 2\n",
      "observation: [-0.59711587 -0.00837157]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 119:\n",
      "action: 1\n",
      "observation: [-0.60494052 -0.00782465]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 120:\n",
      "action: 2\n",
      "observation: [-0.61116115 -0.00622062]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 121:\n",
      "action: 2\n",
      "observation: [-0.61573258 -0.00457143]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 122:\n",
      "action: 2\n",
      "observation: [-0.61862177 -0.00288919]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 123:\n",
      "action: 0\n",
      "observation: [-0.62180791 -0.00318613]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 124:\n",
      "action: 2\n",
      "observation: [-0.62326808 -0.00146018]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 125:\n",
      "action: 1\n",
      "observation: [-0.62399183 -0.00072375]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 126:\n",
      "action: 1\n",
      "observation: [-6.23973960e-01  1.78706986e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 127:\n",
      "action: 1\n",
      "observation: [-0.6232146   0.00075936]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 128:\n",
      "action: 0\n",
      "observation: [-6.22719194e-01  4.95406355e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 129:\n",
      "action: 0\n",
      "observation: [-6.22491292e-01  2.27902087e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 130:\n",
      "action: 2\n",
      "observation: [-0.62053253  0.00195876]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 131:\n",
      "action: 0\n",
      "observation: [-0.61885697  0.00167556]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 132:\n",
      "action: 2\n",
      "observation: [-0.61547665  0.00338031]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 133:\n",
      "action: 1\n",
      "observation: [-0.61141594  0.00406071]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 134:\n",
      "action: 0\n",
      "observation: [-0.6077042   0.00371174]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 135:\n",
      "action: 2\n",
      "observation: [-0.60236834  0.00533586]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 136:\n",
      "action: 1\n",
      "observation: [-0.59644719  0.00592115]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 137:\n",
      "action: 1\n",
      "observation: [-0.58998401  0.00646317]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 138:\n",
      "action: 2\n",
      "observation: [-0.58202623  0.00795778]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 139:\n",
      "action: 1\n",
      "observation: [-0.57363249  0.00839374]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 140:\n",
      "action: 0\n",
      "observation: [-0.5658649   0.00776759]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 141:\n",
      "action: 0\n",
      "observation: [-0.55878116  0.00708374]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 142:\n",
      "action: 0\n",
      "observation: [-0.55243405  0.00634712]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 143:\n",
      "action: 0\n",
      "observation: [-0.54687094  0.00556311]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 144:\n",
      "action: 0\n",
      "observation: [-0.54213343  0.00473751]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 145:\n",
      "action: 1\n",
      "observation: [-0.53725698  0.00487645]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 146:\n",
      "action: 0\n",
      "observation: [-0.53327812  0.00397886]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 147:\n",
      "action: 2\n",
      "observation: [-0.52822668  0.00505144]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 148:\n",
      "action: 1\n",
      "observation: [-0.52314053  0.00508615]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 149:\n",
      "action: 2\n",
      "observation: [-0.51705782  0.00608271]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 150:\n",
      "action: 0\n",
      "observation: [-0.51202416  0.00503366]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 151:\n",
      "action: 0\n",
      "observation: [-0.50807729  0.00394687]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 152:\n",
      "action: 1\n",
      "observation: [-0.5042468  0.0038305]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 153:\n",
      "action: 1\n",
      "observation: [-0.50056136  0.00368544]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 154:\n",
      "action: 2\n",
      "observation: [-0.49604856  0.0045128 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 155:\n",
      "action: 2\n",
      "observation: [-0.49074216  0.0053064 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 156:\n",
      "action: 2\n",
      "observation: [-0.48468178  0.00606038]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 157:\n",
      "action: 0\n",
      "observation: [-0.47991262  0.00476916]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 158:\n",
      "action: 0\n",
      "observation: [-0.47647016  0.00344245]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 159:\n",
      "action: 1\n",
      "observation: [-0.47338     0.00309017]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 160:\n",
      "action: 0\n",
      "observation: [-0.47166505  0.00171495]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 161:\n",
      "action: 0\n",
      "observation: [-4.71338027e-01  3.27019649e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 162:\n",
      "action: 1\n",
      "observation: [-4.71401359e-01 -6.33321825e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 163:\n",
      "action: 1\n",
      "observation: [-4.71854574e-01 -4.53214841e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 164:\n",
      "action: 1\n",
      "observation: [-0.47269431 -0.00083974]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 165:\n",
      "action: 1\n",
      "observation: [-0.47391435 -0.00122004]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 166:\n",
      "action: 1\n",
      "observation: [-0.47550565 -0.0015913 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 167:\n",
      "action: 2\n",
      "observation: [-0.47645639 -0.00095074]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 168:\n",
      "action: 1\n",
      "observation: [-0.47775953 -0.00130313]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 169:\n",
      "action: 2\n",
      "observation: [-0.47840538 -0.00064585]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 170:\n",
      "action: 0\n",
      "observation: [-0.48038913 -0.00198376]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 171:\n",
      "action: 0\n",
      "observation: [-0.48369606 -0.00330692]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 172:\n",
      "action: 0\n",
      "observation: [-0.48830154 -0.00460548]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 173:\n",
      "action: 1\n",
      "observation: [-0.49317126 -0.00486972]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 174:\n",
      "action: 0\n",
      "observation: [-0.49926886 -0.00609761]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 175:\n",
      "action: 1\n",
      "observation: [-0.50554878 -0.00627992]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 176:\n",
      "action: 2\n",
      "observation: [-0.51096401 -0.00541523]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 177:\n",
      "action: 2\n",
      "observation: [-0.51547397 -0.00450996]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 178:\n",
      "action: 0\n",
      "observation: [-0.52104486 -0.00557089]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 179:\n",
      "action: 2\n",
      "observation: [-0.52563491 -0.00459005]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 180:\n",
      "action: 2\n",
      "observation: [-0.52920969 -0.00357478]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 181:\n",
      "action: 1\n",
      "observation: [-0.53274239 -0.0035327 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 182:\n",
      "action: 0\n",
      "observation: [-0.53720652 -0.00446413]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 183:\n",
      "action: 2\n",
      "observation: [-0.54056862 -0.0033621 ]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 184:\n",
      "action: 2\n",
      "observation: [-0.5428035  -0.00223488]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 185:\n",
      "action: 0\n",
      "observation: [-0.54589442 -0.00309093]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 186:\n",
      "action: 0\n",
      "observation: [-0.54981826 -0.00392383]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 187:\n",
      "action: 0\n",
      "observation: [-0.55454565 -0.00472739]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 188:\n",
      "action: 2\n",
      "observation: [-0.55804127 -0.00349562]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 189:\n",
      "action: 2\n",
      "observation: [-0.56027903 -0.00223776]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 190:\n",
      "action: 0\n",
      "observation: [-0.56324224 -0.00296321]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 191:\n",
      "action: 0\n",
      "observation: [-0.56690883 -0.00366659]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 192:\n",
      "action: 0\n",
      "observation: [-0.57125151 -0.00434268]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 193:\n",
      "action: 1\n",
      "observation: [-0.575238  -0.0039865]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 194:\n",
      "action: 2\n",
      "observation: [-0.57783876 -0.00260075]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 195:\n",
      "action: 1\n",
      "observation: [-0.5800345  -0.00219574]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 196:\n",
      "action: 1\n",
      "observation: [-0.58180899 -0.00177449]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 197:\n",
      "action: 2\n",
      "observation: [-5.82149127e-01 -3.40133603e-04]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 198:\n",
      "action: 1\n",
      "observation: [-5.82052387e-01  9.67394238e-05]\n",
      "reward: -1.0\n",
      "done: False\n",
      "info: {}\n",
      "Step 199:\n",
      "action: 0\n",
      "observation: [-5.82519489e-01 -4.67101949e-04]\n",
      "reward: -1.0\n",
      "done: True\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "play_a_random_game_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_data_preparation():\n",
    "    training_data = []\n",
    "    accepted_scores = []\n",
    "    for game_index in range(intial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        previous_observation = []\n",
    "        for step_index in range(goal_steps):\n",
    "            action = random.randrange(0, 3)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            if len(previous_observation) > 0:\n",
    "                game_memory.append([previous_observation, action])\n",
    "                \n",
    "            previous_observation = observation\n",
    "            if observation[0] > -0.25:\n",
    "                reward = 1\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1, 0]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0, 0]\n",
    "                elif data[1] == 2:\n",
    "                    output = [0, 0, 1]\n",
    "                training_data.append([data[0], output])\n",
    "        \n",
    "        env.reset()\n",
    "    \n",
    "    print(accepted_scores)\n",
    "    \n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-172.0, -172.0, -198.0, -170.0, -188.0, -154.0, -198.0, -168.0, -184.0, -180.0, -184.0, -186.0, -168.0, -186.0, -182.0, -190.0, -186.0, -182.0, -172.0, -180.0, -192.0, -198.0, -180.0, -182.0, -176.0, -194.0, -178.0, -194.0, -186.0, -194.0, -188.0, -178.0, -154.0, -178.0, -180.0, -194.0, -188.0, -178.0, -188.0, -174.0, -186.0, -194.0, -178.0, -158.0, -190.0, -168.0, -172.0, -184.0, -174.0, -188.0, -168.0, -184.0, -192.0, -180.0, -176.0, -168.0, -194.0, -176.0, -170.0, -166.0, -182.0, -184.0, -184.0, -182.0, -174.0, -172.0, -186.0, -192.0, -182.0, -184.0, -176.0, -180.0, -182.0, -192.0, -184.0, -178.0, -184.0, -178.0, -170.0, -186.0, -190.0, -172.0, -166.0, -174.0, -172.0, -186.0, -184.0, -174.0, -178.0, -182.0, -198.0, -184.0, -178.0, -172.0, -176.0, -132.0, -194.0, -190.0, -184.0, -170.0, -186.0, -186.0, -184.0, -176.0, -186.0, -170.0, -194.0, -180.0, -176.0, -194.0, -184.0, -166.0, -190.0, -168.0, -192.0, -168.0, -176.0, -174.0, -168.0, -182.0, -158.0, -184.0, -184.0, -164.0, -184.0, -168.0, -184.0, -174.0, -178.0, -176.0, -170.0, -196.0, -192.0, -180.0, -198.0, -168.0, -174.0, -192.0, -174.0, -170.0, -178.0, -190.0, -174.0, -184.0, -184.0, -156.0, -184.0, -176.0, -180.0, -178.0, -190.0, -176.0, -170.0, -164.0, -194.0, -174.0, -190.0, -170.0, -198.0, -190.0, -186.0, -182.0, -174.0, -192.0, -170.0, -196.0, -190.0, -194.0, -172.0, -196.0, -192.0, -190.0, -154.0, -192.0, -196.0, -170.0, -176.0, -170.0, -186.0, -156.0, -184.0, -132.0, -184.0, -166.0, -174.0, -182.0, -188.0, -182.0, -190.0, -188.0, -182.0, -188.0, -194.0, -174.0, -176.0, -160.0, -188.0, -186.0, -196.0, -174.0, -196.0, -146.0, -146.0, -176.0, -172.0, -154.0, -174.0, -182.0, -178.0, -188.0, -190.0, -178.0, -176.0, -168.0, -186.0, -188.0, -178.0, -172.0, -176.0, -184.0, -172.0, -182.0]\n"
     ]
    }
   ],
   "source": [
    "training_data = model_data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, output_size):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=input_size, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(output_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam())\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))\n",
    "    model = build_model(input_size=len(X[0]), output_size=len(y[0]))\n",
    "    \n",
    "    model.fit(X, y, epochs=3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "44178/44178 [==============================] - 2s 48us/step - loss: 0.2231\n",
      "Epoch 2/3\n",
      "44178/44178 [==============================] - 2s 50us/step - loss: 0.2210\n",
      "Epoch 3/3\n",
      "44178/44178 [==============================] - 2s 49us/step - loss: 0.2208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trained_model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = trained_model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"model_AIGame_MC_v2.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()\n",
    "trained_model.save_weights(\"model_AIGame_MC_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-164.0, -92.0, -162.0, -87.0, -173.0, -167.0, -117.0, -98.0, -173.0, -168.0, -96.0, -165.0, -105.0, -91.0, -91.0, -164.0, -90.0, -101.0, -164.0, -95.0, -168.0, -164.0, -106.0, -162.0, -169.0, -164.0, -167.0, -164.0, -200.0, -170.0, -115.0, -163.0, -184.0, -185.0, -105.0, -162.0, -165.0, -112.0, -181.0, -95.0, -129.0, -165.0, -163.0, -175.0, -167.0, -176.0, -98.0, -109.0, -88.0, -164.0, -106.0, -96.0, -162.0, -100.0, -92.0, -167.0, -162.0, -88.0, -200.0, -102.0, -164.0, -185.0, -89.0, -167.0, -108.0, -162.0, -111.0, -170.0, -92.0, -171.0, -164.0, -104.0, -162.0, -186.0, -170.0, -164.0, -179.0, -89.0, -176.0, -164.0, -169.0, -168.0, -192.0, -183.0, -88.0, -163.0, -168.0, -166.0, -89.0, -96.0, -92.0, -104.0, -164.0, -87.0, -195.0, -91.0, -87.0, -119.0, -164.0, -185.0]\n",
      "Average Score: -141.2\n",
      "choice 1:0.032577903682719546  choice 0:0.4296742209631728 choice 2:0.5377478753541076\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "choices = []\n",
    "for each_game in range(100):\n",
    "    score = 0\n",
    "    prev_obs = []\n",
    "    for step_index in range(goal_steps):\n",
    "        # Uncomment this line if you want to see how our bot playing\n",
    "        env.render()\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,2)\n",
    "        else:\n",
    "            action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])\n",
    "        \n",
    "        choices.append(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        prev_obs = new_observation\n",
    "        score+=reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.reset()\n",
    "    scores.append(score)\n",
    "\n",
    "print(scores)\n",
    "print('Average Score:',sum(scores)/len(scores))\n",
    "print('choice 1:{}  choice 0:{} choice 2:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices),choices.count(2)/len(choices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
